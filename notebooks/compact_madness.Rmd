---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.17.2
  kernelspec:
    display_name: birder
    language: python
    name: birder
---

# Compact Madness

Compare compact models on multiple Birder data sets.


## Imports and Configuration

```{python}
import sys
from pathlib import Path

import altair as alt
import polars as pl
```

```{python}
sys.path.append("..")
```

```{python}
from birder.common import cli
from birder.conf import settings
from birder.model_registry import registry
from birder.net.base import reparameterize_available
from birder.results.classification import Results
from birder.results.classification import compare_results
from birder.tools.model_info import get_model_info
```

```{python}
# %cd ..
```

```{python}
PATTERN = "danube-delta"
RESULTS_PATH = PATTERN
```

## Helper Functions

```{python}
def get_model_name_and_num_parameters(
    file_name: str, tag: str = PATTERN
) -> tuple[str, str, int, bool, bool, bool, bool]:
    reparameterized = False
    intermediate = False
    mim = False
    dist = False
    ssl = False
    prop_list = []

    def mark_ssl(ssl_name: str) -> bool:
        nonlocal file_name
        nonlocal prop_list

        if file_name.find(ssl_name) != -1:
            file_name = file_name.replace(f"_{ssl_name}", "")
            file_name = file_name.replace(f"-{ssl_name}", "")
            if "ssl" not in prop_list:
                prop_list.append("ssl")

            return True

        return False

    for ssl_name in [
        "barlow-twins",
        "byol",
        "capi",
        "data2vec",
        "dino-v1",
        "dino-v2",
        "dino-v2-dist",
        "i-jepa",
        "ibot",
        "mmcr",
        "simclr",
        "vicreg",
    ]:
        ssl |= mark_ssl(ssl_name)

    if file_name.find("reparameterized") != -1:
        reparameterized = True
        file_name = file_name.replace("_reparameterized", "")
        prop_list.append("r")
    if file_name.find("mim") != -1:
        mim = True
        file_name = file_name.replace("_mim", "")
        file_name = file_name.replace("-mim", "")
        prop_list.append("mim")
    if file_name.find("dist") != -1:
        dist = True
        file_name = file_name.replace("_dist", "")
        file_name = file_name.replace("-dist", "")
        prop_list.append("d")
    if file_name.find("intermediate") != -1:
        intermediate = True
        file_name = file_name.replace("_intermediate", "")
        file_name = file_name.replace("-intermediate", "")
        prop_list.append("i")

    file_name = file_name.split("/")[-1]
    tag_end_idx = file_name.find("_", file_name.find(tag))
    num_classes_str = file_name[tag_end_idx + 1 :]
    num_classes = int(num_classes_str.split("_")[0])
    model_name = file_name[: file_name.find(tag) - 1]
    model = registry.net_factory(model_name, 3, num_classes=num_classes)

    if reparameterize_available(model) is True and reparameterized is True:
        model.reparameterize_model()

    if len(prop_list) > 0:
        model_name += " (" + ", ".join(prop_list) + ")"

    info = get_model_info(model)
    if "model_registry.model_registry" in model.__class__.__module__:
        model_type = model.__class__.__bases__[0].__name__
    else:
        model_type = model.__class__.__name__

    return (
        model_name.replace("_", " "),
        model_type.replace("_", " "),
        info["num_params"],
        intermediate,
        mim,
        dist,
        ssl,
    )
```

```{python}
def get_resolution(file_name: str, tag: str = PATTERN) -> int:
    tag_end_idx = file_name.find("_", file_name.find(tag))
    file_name = file_name[: file_name.find("px", tag_end_idx)]
    resolution = file_name.rsplit("_", 1)[1]

    return int(resolution)
```

```{python}
def get_pretrained_name(file_name: str) -> str:
    if file_name.find("reparameterized") != -1:
        pattern = "reparameterized"
    else:
        pattern = PATTERN

    pretrained_name = file_name[: file_name.find(pattern) + len(pattern)]

    return pretrained_name
```

## Data Loading and Processing

```{python}
# results_dict = {}
# for result_file in settings.RESULTS_DIR.joinpath(RESULTS_PATH).glob(f"*{PATTERN}*"):
#     results_dict[result_file.stem] = Results.load(result_file)

# compare_results_df = compare_results(results_dict)
compare_results_df = pl.read_csv(f"results/summary_{PATTERN}.csv")
```

```{python}
num_param = []
model_names = []
model_types = []
pretrained_names = []
resolution = []
intermediate = []
mim = []
dist = []
ssl = []
for file_name in compare_results_df["File name"]:
    (model_name, model_type, count, intermediate_trn, mim_trn, dist_trn, ssl_trn) = get_model_name_and_num_parameters(
        file_name
    )
    num_param.append(count / 1e6)
    model_names.append(model_name)
    model_types.append(model_type)
    pretrained_names.append(get_pretrained_name(file_name))
    resolution.append(get_resolution(file_name))
    intermediate.append(intermediate_trn)
    mim.append(mim_trn)
    dist.append(dist_trn)
    ssl.append(ssl_trn)

compare_results_df = compare_results_df.with_columns(
    pl.Series("Parameters (M)", num_param),
    pl.Series("Resolution", resolution),
    pl.Series("Model name", model_names),
    pl.Series("Model type", model_types),
    pl.Series("Pretrained name", pretrained_names),
    pl.Series("Intermediate", intermediate),
    pl.Series("MIM", mim),
    pl.Series("Distilled", dist),
    pl.Series("Self-supervised learning", ssl),
)
```

```{python}
# Drop non-reparameterized versions when both available
df = compare_results_df.with_columns(
    [
        pl.col("Model name").str.replace(" \(r\)$", "").alias("base_name"),
        pl.col("Model name").str.ends_with(" (r)").alias("has_r_suffix"),
    ]
)
df = df.filter(df[["base_name", "Resolution"]].is_duplicated()).sort(
    by=["base_name", "has_r_suffix"], descending=[False, False]
)
names_to_drop = df.unique("base_name", keep="first")["File name"]
compare_results_df = compare_results_df.filter(~pl.col("File name").is_in(names_to_drop.implode()))
```

### Join Benchmark Data

```{python}
benchmark_df = pl.read_csv(settings.RESULTS_DIR.joinpath(f"benchmark_{PATTERN}.csv"))
benchmark_df = benchmark_df.with_columns(pl.col("model_name").str.replace(r"\d+px$", ""))
compare_results_df = compare_results_df.join(
    benchmark_df, left_on=["Pretrained name", "Resolution"], right_on=["model_name", "size"], how="left"
)
compare_results_df = compare_results_df.with_columns(
    (1000 * (1 / pl.col("samples_per_sec"))).alias("ms / sample"),
)

# Filter relevant benchmark data
# compare_results_df = compare_results_df.filter((pl.col("ms / sample").is_null() & pl.col("peak_memory").is_null()))
```

```{python}
# Rearrange columns
compare_results_df = compare_results_df.select(
    "Model name",
    "Model type",
    pl.all().exclude(
        "File name", "Model name", "Model type", "Pretrained name", "Parameters (M)", *benchmark_df.columns
    ),
    "Parameters (M)",
    "samples_per_sec",
    "peak_memory",
    "device",
    "max_batch_size",
    "compile",
    "amp",
    "single_thread",
    "torch_version",
)
compare_results_df = compare_results_df.rename(
    {"samples_per_sec": "Samples / sec", "peak_memory": "Peak GPU memory (MB)"}
)
```

### Save Processed Output

```{python}
compare_results_df.write_csv(settings.RESULTS_DIR.joinpath(f"results_{PATTERN}.csv"))
```

## Visualize Results

```{python}
def plot_acc_param(param_compare_results_df: pl.DataFrame, width: int = 900, height: int = 640) -> alt.LayerChart:
    df = param_compare_results_df.select(
        "Model name",
        "Model type",
        "Accuracy",
        "Top-3 accuracy",
        "Resolution",
        "Parameters (M)",
        "Pareto frontier (p)",
        "Intermediate",
        "MIM",
        "Distilled",
    )
    base = df.plot.point(
        x="Parameters (M)",
        y="Accuracy",
        color="Model type",
        shape="Resolution:N",
        tooltip=[
            "Parameters (M)",
            "Accuracy",
            "Top-3 accuracy",
            "Model name",
            "Model type",
            "Resolution",
            "Intermediate",
            "MIM",
            "Distilled",
        ],
    )
    text = base.mark_text(align="center", baseline="middle", dy=-10).encode(text="Model name")
    frontier = df.plot.line(x="Parameters (M)", y="Pareto frontier (p)").mark_line(
        interpolate="step-after", color="red", strokeWidth=0.3, strokeDash=(2, 2)
    )

    chart = base + text + frontier
    return chart.properties(title="Accuray vs Parameter Count", width=width, height=height).configure_scale(zero=False)
```

```{python}
def plot_acc_memory(memory_compare_results_df: pl.DataFrame, width: int = 900, height: int = 640) -> alt.LayerChart:
    batch_size = memory_compare_results_df["max_batch_size"][0]
    amp = memory_compare_results_df["amp"][0]
    df = memory_compare_results_df.select(
        "Model name",
        "Model type",
        "Accuracy",
        "Top-3 accuracy",
        "Resolution",
        "Peak GPU memory (MB)",
        "Parameters (M)",
        "Pareto frontier (mem)",
        "Intermediate",
        "MIM",
        "Distilled",
    )
    base = df.plot.point(
        x="Peak GPU memory (MB)",
        y="Accuracy",
        color="Model type",
        shape="Resolution:N",
        tooltip=[
            "Peak GPU memory (MB)",
            "Parameters (M)",
            "Accuracy",
            "Top-3 accuracy",
            "Model name",
            "Model type",
            "Resolution",
            "Intermediate",
            "MIM",
            "Distilled",
        ],
    )
    text = base.mark_text(align="center", baseline="middle", dy=-10).encode(text="Model name")
    frontier = df.plot.line(x="Peak GPU memory (MB)", y="Pareto frontier (mem)").mark_line(
        interpolate="step-after", color="red", strokeWidth=0.3, strokeDash=(2, 2)
    )

    chart = base + text + frontier
    return chart.properties(
        title=f"Accuray vs GPU Memory (batch size={batch_size}, amp={amp})", width=width, height=height
    ).configure_scale(zero=False)
```

```{python}
def plot_acc_rate(rate_compare_results_df: pl.DataFrame, width: int = 900, height: int = 640) -> alt.LayerChart:
    device = rate_compare_results_df["device"][0]
    compiled = rate_compare_results_df["compile"][0]
    batch_size = rate_compare_results_df["max_batch_size"][0]
    amp = rate_compare_results_df["amp"][0]
    df = rate_compare_results_df.select(
        "Model name",
        "Model type",
        "Accuracy",
        "Top-3 accuracy",
        "Resolution",
        "ms / sample",
        "Parameters (M)",
        "Pareto frontier (ms)",
        "Intermediate",
        "MIM",
        "Distilled",
    )
    base = df.plot.point(
        x="ms / sample",
        y="Accuracy",
        color="Model type",
        shape="Resolution:N",
        tooltip=[
            "ms / sample",
            "Parameters (M)",
            "Accuracy",
            "Top-3 accuracy",
            "Model name",
            "Model type",
            "Resolution",
            "Intermediate",
            "MIM",
            "Distilled",
        ],
    )
    text = base.mark_text(align="center", baseline="middle", dy=-10).encode(text="Model name")
    frontier = df.plot.line(x="ms / sample", y="Pareto frontier (ms)").mark_line(
        interpolate="step-after", color="red", strokeWidth=0.3, strokeDash=(2, 2)
    )

    chart = base + text + frontier
    return chart.properties(
        title=f"Accuray vs {device.upper()} Rate (compile={compiled}, batch size={batch_size}, amp={amp})",
        width=width,
        height=height,
    ).configure_scale(zero=False)
```

```{python}
compare_results_df = pl.read_csv(settings.RESULTS_DIR.joinpath(f"results_{PATTERN}.csv"))

# Parameter count
param_compare_results_df = compare_results_df.unique(subset=["Model name", "Resolution"]).sort(
    "Parameters (M)", descending=False
)
param_compare_results_df = param_compare_results_df.with_columns(
    pl.col("Accuracy").cum_max().alias("Pareto frontier (p)")
)
param_compare_results_df = param_compare_results_df.drop("Samples / sec", "device", "ms / sample")
chart = plot_acc_param(param_compare_results_df)
# chart.layer[0].encoding.x.scale = alt.Scale(domain=[0.5, 7.5])
x_max = param_compare_results_df["Parameters (M)"].quantile(0.9)
x_min = param_compare_results_df["Parameters (M)"].quantile(0.1)
chart.layer[0].encoding.x.scale = alt.Scale(domain=[x_min, x_max])
chart.show()
```

```{python}
# Memory usage
memory_compare_results_df = compare_results_df.drop_nulls(subset=["Peak GPU memory (MB)"])
memory_compare_results_df = memory_compare_results_df.unique(subset=["Model name", "Resolution"]).sort(
    "Peak GPU memory (MB)", descending=False
)
memory_compare_results_df = memory_compare_results_df.with_columns(
    pl.col("Accuracy").cum_max().alias("Pareto frontier (mem)")
)
memory_compare_results_df = memory_compare_results_df.drop("Samples / sec", "device", "ms / sample")
chart = plot_acc_memory(memory_compare_results_df)
x_max = memory_compare_results_df["Peak GPU memory (MB)"].quantile(0.9)
x_min = memory_compare_results_df["Peak GPU memory (MB)"].quantile(0.1)
chart.layer[0].encoding.x.scale = alt.Scale(domain=[x_min, x_max])
chart.show()
```

```{python}
combination_df = (
    compare_results_df.drop_nulls(subset=["ms / sample"])
    .select("device", "max_batch_size", "compile", "amp", "single_thread")
    .unique()
)
combination_df = combination_df.drop_nulls()
for combination in combination_df.iter_rows(named=True):
    df = compare_results_df.filter(**combination)

    # Device rate
    device_compare_results_df = df.unique(subset=["Model name", "Resolution"]).sort("ms / sample", descending=False)
    device_compare_results_df = device_compare_results_df.with_columns(
        pl.col("Accuracy").cum_max().alias("Pareto frontier (ms)")
    )
    chart = plot_acc_rate(device_compare_results_df)
    # chart.show()

    x_max = device_compare_results_df["ms / sample"].quantile(0.95) * 1.04
    x_min = device_compare_results_df["ms / sample"].min() * 0.96
    chart.layer[0].encoding.x.scale = alt.Scale(domain=[x_min, x_max])
    chart.show()
```
