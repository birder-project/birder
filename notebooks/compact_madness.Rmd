---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.7
  kernelspec:
    display_name: birder
    language: python
    name: birder
---

# Compact Madness

Compare compact models on multiple Birder data sets.


## Imports and Configuration

```{python}
import sys
from pathlib import Path

import altair as alt
import polars as pl
```

```{python}
sys.path.append("..")
```

```{python}
from birder.common import cli
from birder.conf import settings
from birder.model_registry import registry
from birder.net.base import reparameterize_available
from birder.results.classification import Results
from birder.results.classification import compare_results
from birder.tools.model_info import get_model_info
```

```{python}
# %cd ..
```

```{python}
PATTERN = "il-common"
RESULTS_PATH = "il-common-v1"
```

## Helper Functions

```{python}
def get_model_name_and_num_parameters(file_name: str, tag: str = PATTERN) -> tuple[str, str, int, bool, bool]:
    reparameterized = False
    intermediate = False
    mim = False
    if file_name.find("reparameterized") != -1:
        reparameterized = True
        file_name = file_name.replace("_reparameterized", "")
    if file_name.find("intermediate") != -1:
        intermediate = True
        file_name = file_name.replace("_intermediate", "")
        file_name = file_name.replace("-intermediate", "")
    if file_name.find("mim") != -1:
        mim = True
        file_name = file_name.replace("_mim", "")
        file_name = file_name.replace("-mim", "")

    file_name = file_name.split("/")[-1]
    num_classes_str = file_name[file_name.find(tag) + len(tag) + 1 :]
    num_classes = int(num_classes_str.split("_")[0])
    model_name = file_name[: file_name.find(tag) - 1]
    if model_name in registry.all_nets:
        net_param = None
        model = registry.net_factory(model_name, 3, num_classes=num_classes)
    else:
        (model_name, net_param) = model_name.rsplit("_", 1)
        model = registry.net_factory(model_name, 3, num_classes=num_classes, net_param=float(net_param))
        model_name += f"_{net_param}"

    if reparameterize_available(model) is True and reparameterized is True:
        model.reparameterize_model()
        model_name += " (r)"

    info = get_model_info(model)
    if "model_registry.model_registry" in model.__class__.__module__:
        model_type = model.__class__.__bases__[0].__name__
    else:
        model_type = model.__class__.__name__

    return (model_name.replace("_", " "), model_type.replace("_", " "), info["num_params"], intermediate, mim)
```

```{python}
def get_resolution(file_name: str) -> int:
    file_name = file_name[: file_name.find("px")]
    resolution = file_name.rsplit("_", 1)[1]

    return int(resolution)
```

```{python}
def get_pretrained_name(file_name: str) -> str:
    if file_name.find("reparameterized") != -1:
        pattern = "reparameterized"
    else:
        pattern = PATTERN

    pretrained_name = file_name[: file_name.find(pattern) + len(pattern)]

    return pretrained_name
```

## Data Loading and Processing

```{python}
# results_dict = {}
# for result_file in settings.RESULTS_DIR.joinpath(RESULTS_PATH).glob(f"*{PATTERN}*"):
#     results_dict[result_file.stem] = Results.load(result_file)

# compare_results_df = compare_results(results_dict)
compare_results_df = pl.read_csv(f"results/summary_{PATTERN}.csv")
```

```{python}
num_param = []
model_names = []
model_types = []
pretrained_names = []
resolution = []
intermediate = []
mim = []
for file_name in compare_results_df["File name"]:
    (model_name, model_type, count, intermediate_trn, mim_trn) = get_model_name_and_num_parameters(file_name)
    num_param.append(count / 1e6)
    model_names.append(model_name)
    model_types.append(model_type)
    pretrained_names.append(get_pretrained_name(file_name))
    resolution.append(get_resolution(file_name))
    intermediate.append(intermediate_trn)
    mim.append(mim_trn)

compare_results_df = compare_results_df.with_columns(
    pl.Series("Parameters (M)", num_param),
    pl.Series("Resolution", resolution),
    pl.Series("Model name", model_names),
    pl.Series("Model type", model_types),
    pl.Series("Pretrained name", pretrained_names),
    pl.Series("Intermediate", intermediate),
    pl.Series("MIM", mim),
)
```

```{python}
# Drop non-reparameterized versions when both available
df = compare_results_df.with_columns(
    [
        pl.col("Model name").str.replace(" \(r\)$", "").alias("base_name"),
        pl.col("Model name").str.ends_with(" (r)").alias("has_r_suffix"),
    ]
)
df = df.filter(df["base_name"].is_duplicated()).sort(by=["base_name", "has_r_suffix"], descending=[False, False])
names_to_drop = df.unique("base_name", keep="first")["File name"]
compare_results_df = compare_results_df.filter(~pl.col("File name").is_in(names_to_drop))
```

### Join Benchmark Data

```{python}
benchmark_df = pl.read_csv(settings.RESULTS_DIR.joinpath(f"benchmark_{PATTERN}.csv"))
compare_results_df = compare_results_df.join(benchmark_df, left_on="Pretrained name", right_on="model_name", how="left")
compare_results_df = compare_results_df.with_columns(
    (1000 * (1 / pl.col("samples_per_sec"))).alias("ms / sample"),
)

# Filter relevant benchmark data
compare_results_df = compare_results_df.filter(
    (pl.col("Resolution") == pl.col("size")) | (pl.col("ms / sample").is_null())  # Not benchmarked yet
)
```

```{python}
# Rearrange columns
compare_results_df = compare_results_df.select(
    "Model name",
    "Model type",
    pl.all().exclude(
        "File name", "Model name", "Model type", "Pretrained name", "Parameters (M)", *benchmark_df.columns
    ),
    "Parameters (M)",
    "samples_per_sec",
    "device",
    "batch_size",
    "compile",
    "amp",
)
compare_results_df = compare_results_df.rename({"samples_per_sec": "Samples / sec"})
```

### Save Processed Output

```{python}
compare_results_df.write_csv(settings.RESULTS_DIR.joinpath(f"results_{PATTERN}.csv"))
```

## Visualize Results

```{python}
def plot_acc_param(param_compare_results_df: pl.DataFrame, width: int = 900, height: int = 640) -> alt.LayerChart:
    df = param_compare_results_df.select(
        "Model name", "Model type", "Accuracy", "Top-3 accuracy", "Resolution", "Parameters (M)", "Pareto frontier (p)"
    )
    base = df.plot.point(
        x="Parameters (M)",
        y="Accuracy",
        color="Model type",
        shape="Resolution:N",
        tooltip=["Parameters (M)", "Accuracy", "Top-3 accuracy", "Model name", "Model type", "Resolution"],
    )
    text = base.mark_text(align="center", baseline="middle", dy=-10).encode(text="Model name")
    frontier = df.plot.line(x="Parameters (M)", y="Pareto frontier (p)").mark_line(
        interpolate="step-after", color="red", strokeWidth=0.3, strokeDash=(2, 2)
    )

    chart = base + text + frontier
    return chart.properties(title="Accuray vs Parameter Count", width=width, height=height).configure_scale(zero=False)
```

```{python}
def plot_acc_rate(rate_compare_results_df: pl.DataFrame, width: int = 900, height: int = 640) -> alt.LayerChart:
    device = rate_compare_results_df["device"][0]
    compiled = rate_compare_results_df["compile"][0]
    batch_size = rate_compare_results_df["batch_size"][0]
    amp = rate_compare_results_df["amp"][0]
    df = rate_compare_results_df.select(
        "Model name",
        "Model type",
        "Accuracy",
        "Top-3 accuracy",
        "Resolution",
        "ms / sample",
        "Parameters (M)",
        "Pareto frontier (ms)",
    )
    base = df.plot.point(
        x="ms / sample",
        y="Accuracy",
        color="Model type",
        shape="Resolution:N",
        tooltip=[
            "ms / sample",
            "Parameters (M)",
            "Accuracy",
            "Top-3 accuracy",
            "Model name",
            "Model type",
            "Resolution",
        ],
    )
    text = base.mark_text(align="center", baseline="middle", dy=-10).encode(text="Model name")
    frontier = df.plot.line(x="ms / sample", y="Pareto frontier (ms)").mark_line(
        interpolate="step-after", color="red", strokeWidth=0.3, strokeDash=(2, 2)
    )

    chart = base + text + frontier
    return chart.properties(
        title=f"Accuray vs {device.upper()} Rate (compile={compiled}, batch size={batch_size}, amp={amp})",
        width=width,
        height=height,
    ).configure_scale(zero=False)
```

```{python}
compare_results_df = pl.read_csv(settings.RESULTS_DIR.joinpath(f"results_{PATTERN}.csv"))

# Parameter count
param_compare_results_df = compare_results_df.unique(subset=["Model name"]).sort("Parameters (M)", descending=False)
param_compare_results_df = param_compare_results_df.with_columns(
    pl.col("Accuracy").cum_max().alias("Pareto frontier (p)")
)
param_compare_results_df = param_compare_results_df.drop("Samples / sec", "device", "ms / sample")
chart = plot_acc_param(param_compare_results_df)
# chart.show()
chart.layer[0].encoding.x.scale = alt.Scale(domain=[0.5, 7.5])
chart.show()
```

```{python}
combination_df = compare_results_df.select("device", "batch_size", "compile", "amp").unique()
combination_df = combination_df.drop_nulls()
for combination in combination_df.iter_rows(named=True):
    df = compare_results_df.filter(**combination)

    # Device rate
    device_compare_results_df = df.unique(subset=["Model name"]).sort("ms / sample", descending=False)
    device_compare_results_df = device_compare_results_df.with_columns(
        pl.col("Accuracy").cum_max().alias("Pareto frontier (ms)")
    )
    chart = plot_acc_rate(device_compare_results_df)
    # chart.show()

    x_max = device_compare_results_df["ms / sample"].quantile(0.95) * 1.04
    x_min = device_compare_results_df["ms / sample"].min() * 0.96
    chart.layer[0].encoding.x.scale = alt.Scale(domain=[x_min, x_max])
    chart.show()
```
